% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CS_2Step_Master.R
\name{strategize}
\alias{strategize}
\title{Estimate Optimal (or Adversarial) Stochastic Interventions for Conjoint Experiments}
\usage{
strategize(
  Y,
  W,
  X = NULL,
  lambda,
  varcov_cluster_variable = NULL,
  competing_group_variable_respondent = NULL,
  competing_group_variable_respondent_proportions = NULL,
  competing_group_variable_candidate = NULL,
  competing_group_competition_variable_candidate = NULL,
  pair_id = NULL,
  respondent_id = NULL,
  respondent_task_id = NULL,
  profile_order = NULL,
  p_list = NULL,
  slate_list = NULL,
  K = 1,
  nSGD = 100,
  diff = FALSE,
  adversarial = FALSE,
  use_regularization = TRUE,
  force_gaussian = FALSE,
  a_init_sd = 0.001,
  outcome_model_type = "glm",
  penalty_type = "KL",
  compute_se = FALSE,
  conda_env = "strategize_env",
  conda_env_required = FALSE,
  conf_level = 0.90,
  nFolds_glm = 3L,
  folds = NULL,
  nMonte_adversarial = 5L,
  nMonte_Qglm = 100L,
  learning_rate_max = 0.001,
  temperature = 0.5,
  save_outcome_model = FALSE,
  presaved_outcome_model = FALSE,
  use_optax = FALSE,
  optim_type = "gd"
)
}
\arguments{
\item{Y}{A numeric or binary vector of observed outcomes, typically in \code{\{0,1\}} for forced-choice
conjoint tasks, indicating whether the profile was selected. For instance, \code{Y = 1} if
candidate A was chosen over candidate B, and \code{Y = 0} otherwise. The length must match
the number of rows in \code{W}.}

\item{W}{A matrix or data frame representing the assigned levels of each factor in a conjoint
design (one column per factor). Each row corresponds to a single profile. For forced-choice
tasks, a given respondent may have contributed multiple rows if you reshape pairwise choices
into long format. If the experiment used multiple factors \eqn{D}, with each factor having
\eqn{L_d} levels, \code{W} should capture all factor assignments accordingly.}

\item{X}{An optional matrix or data frame of additional covariates, often respondent-level
features (e.g., respondent demographics). If \code{K > 1}, \code{X} may be used internally to
fit multi-cluster or multi-component outcome models, or to allow cluster-specific effect
estimation for more granular insights. Defaults to \code{NULL}.}

\item{lambda}{A numeric scalar or vector giving the regularization penalty (e.g., in Kullback-Leibler
or L2 sense) used to shrink the learned probability distribution(s) of factor levels toward a
baseline distribution \code{p_list}. Typically set via either domain knowledge or cross-validation.}

\item{varcov_cluster_variable}{An optional vector of cluster identifiers (e.g., respondent IDs)
used to form a robust variance-covariance estimate of the outcome model. If \code{NULL},
the usual IID assumption is made. Defaults to \code{NULL}.}

\item{competing_group_variable_respondent}{Optional variable marking competition group
membership of respondents. Particularly relevant in adversarial settings
(\code{adversarial = TRUE}) or multi-stage electoral settings, e.g., capturing the party of each
respondent. Defaults to \code{NULL}.}

\item{competing_group_variable_respondent_proportions}{Optional numeric vector specifying
the population proportions of each competing group. If \code{NULL}, proportions are estimated
from the data. Useful when the sample proportions differ from the target population proportions.
Defaults to \code{NULL}.}

\item{competing_group_variable_candidate}{Optional variable marking competition group
membership of candidate profiles. Defaults to \code{NULL}.}

\item{competing_group_competition_variable_candidate}{Optional variable indicating whether
a candidate profile belongs to the competing group in adversarial settings. Defaults to \code{NULL}.}

\item{pair_id}{A factor or numeric vector identifying the forced-choice pair. If each row of
\code{W} is a single profile, \code{pair_id} groups the rows belonging to the same choice set.
Defaults to \code{NULL}.}

\item{respondent_id, respondent_task_id}{Another set of optional identifiers. \code{respondent_id}
marks each respondent across tasks, while \code{respondent_task_id} can define unique IDs for
repeated measurements from the same respondent across multiple tasks. Useful for advanced
clustering or robust SEs. Defaults to \code{NULL}.}

\item{profile_order}{If each forced-choice is shown with different ordering (e.g., \verb{Candidate A}
vs. \verb{Candidate B}), \code{profile_order} can label each row accordingly. Helpful for ensuring
consistent labeling of reference vs. opposing profiles. Defaults to \code{NULL}.}

\item{p_list}{An optional list describing the baseline probability distribution over factor levels
in \code{W}. Typically derived from the initial design distribution or uniform assignment
distribution. If \code{NULL}, the function may assume uniform or attempt to estimate the
distribution from \code{W}.}

\item{slate_list}{An optional list (or lists) providing custom “slates” of candidate features
(and their associated probabilities). Used in more advanced or adversarial setups where
certain combinations must be included or excluded. If \code{NULL}, no special constraints
beyond the usual factor-level distributions are applied.}

\item{K}{Integer specifying the number of latent clusters for multi-component outcome models. If
\code{K = 1}, no latent clustering is done. Defaults to \code{1}.}

\item{nSGD}{Integer specifying the number of stochastic gradient descent (or gradient-based)
iterations to use when learning the optimal distributions. Defaults to \code{100}.}

\item{diff}{Logical indicating whether the outcome \code{Y} represents a first-difference or
difference-based metric. In forced-choice contexts, typically \code{diff = FALSE}. Defaults
to \code{FALSE}.}

\item{adversarial}{Logical controlling whether to enable the max-min adversarial scenario. When
\code{TRUE}, the function searches for a pair of distributions (one for each competing party
or group) such that each party’s distribution is optimal given the other party’s distribution.
Defaults to \code{FALSE}.}

\item{use_regularization}{Logical indicating whether to regularize the outcome model (in addition
to any penalty \code{lambda} on the distribution shift). This can help avoid overfitting in
high-dimensional designs. Defaults to \code{TRUE}.}

\item{force_gaussian}{Logical indicating whether to force a Gaussian-based outcome modeling
approach, even if \code{Y} is binary or forced-choice. If \code{FALSE}, the function attempts
to choose a more appropriate link (e.g., \code{"binomial"}). Defaults to \code{FALSE}.}

\item{a_init_sd}{Numeric scalar specifying the standard deviation for random initialization
of unconstrained parameters used in the gradient-based search over factor-level probabilities.
Defaults to \code{0.001}.}

\item{outcome_model_type}{Character string specifying the outcome model to use. Currently
supports \code{"glm"} for generalized linear models or \code{"neural"} for a neural-network
approximation. Defaults to \code{"glm"}.}

\item{penalty_type}{A character string specifying the type of penalty (e.g., \code{"KL"}, \code{"L2"},
or \code{"LogMaxProb"}) used in the objective function for shifting the factor-level probabilities
away from the baseline \code{p_list}. Defaults to \code{"KL"}.}

\item{compute_se}{Logical indicating whether standard errors should be computed for the final
estimates (via the delta method or related expansions). Defaults to \code{FALSE}.}

\item{conda_env}{A character string naming a Python conda environment that includes \pkg{jax},
\pkg{optax}, and other dependencies. If not \code{NULL}, the function attempts to activate
that environment. Defaults to \code{"strategize_env"}.}

\item{conda_env_required}{Logical; if \code{TRUE}, raises an error if the environment given by
\code{conda_env} cannot be activated. Otherwise, the function attempts to proceed with any
available installation. Defaults to \code{FALSE}.}

\item{conf_level}{Numeric in \eqn{(0,1)}, specifying the confidence level for intervals or
credible bounds. Defaults to \code{0.90}.}

\item{nFolds_glm}{Integer specifying the number of folds (default \code{3L}) for internal
cross-validation used in certain outcome model or regularization steps. Defaults to \code{3L}.}

\item{folds}{An optional user-supplied partitioning or CV scheme, overriding \code{nFolds_glm}.
Defaults to \code{NULL}.}

\item{nMonte_adversarial}{Integer specifying the number of Monte Carlo samples used in adversarial
or max-min steps, e.g., sampling from the opposing candidate’s distribution to approximate
expected payoffs. Defaults to \code{5L}.}

\item{nMonte_Qglm}{Integer specifying the number of Monte Carlo samples for evaluating or
approximating the quantity of interest under certain outcomes or distributions. Defaults to
\code{100L}.}

\item{learning_rate_max}{Base learning rate for gradient-based optimizers. Defaults to
\code{0.001}.}

\item{temperature}{Numeric temperature parameter used in Gumbel-Softmax sampling to smooth
the exploration of the probability simplex. Smaller values yield distributions closer to the
argmax. Defaults to \code{0.5}.}

\item{save_outcome_model}{Logical indicating whether to save the fitted outcome model to
disk for reuse. Useful for large models or repeated runs. Defaults to \code{FALSE}.}

\item{presaved_outcome_model}{Logical indicating whether to use a previously saved outcome
model instead of re-fitting. Defaults to \code{FALSE}.}

\item{use_optax}{Logical indicating whether to use the \href{https://github.com/deepmind/optax}{\code{optax}}
library for gradient-based optimization in JAX (\code{TRUE}) or a built-in method (\code{FALSE}).
Defaults to \code{FALSE}.}

\item{optim_type}{A character string for choosing which optimizer or approach is used internally
(e.g., \code{"gd"} for gradient descent). Defaults to \code{"gd"}.}
}
\value{
A named \code{list} containing:
\describe{
\item{\code{pi_star_point}}{An estimate of the (possibly multi-cluster or adversarial)
optimal distribution(s) over the factor levels.

Structure depends on parameters:
\itemize{
\item If \code{adversarial = TRUE} and \code{K = 1}, returns a pair of distributions (e.g., maximin solutions).
\item If \code{K > 1}, returns a list where each element corresponds to a cluster-optimal distribution.
\item Otherwise, returns a single distribution.}
}

\item{\code{pi_star_se}}{Standard errors for entries in \code{pi_star_point}. Mirrors the structure of \code{pi_star_point} (e.g., a pair of SEs if \code{adversarial = TRUE} and \code{K = 1}). Only present if \code{compute_se = TRUE}.}

\item{\code{Q_point_mEst}}{Point estimate(s) of the optimized outcome (e.g., utility/vote share). Matches the structure of \code{pi_star_point}.}

\item{\code{Q_se_mEst}}{Standard errors for \code{Q_point_mEst}. Only present if \code{compute_se = TRUE}.}

\item{\code{pi_star_lb}, \code{pi_star_ub}}{Confidence bounds for \code{pi_star_point} (if \code{compute_se = TRUE} and a confidence level is provided).}

\item{\code{CVInfo}}{Cross-validation performance data (if applicable). Typically a \code{data.frame} or list.}

\item{\code{estimationType}}{String indicating the approach used (e.g., \code{"TwoStep"} or \code{"OneStep"}).}

\item{\code{...}}{Additional internal details (e.g., fitted models, optimization logs).}
}
}
\description{
\code{strategize} implements the core methods described in the accompanying paper
for learning an optimal or adversarial probability distribution over conjoint factor levels.
It is specifically designed for forced-choice conjoint settings (e.g., candidate-choice experiments)
and can accommodate scenarios in which a single agent optimizes its strategy in isolation,
or in which two (potentially adversarial) agents simultaneously optimize against each other.

This function can be used to find the \emph{optimal stochastic intervention} for maximizing
an outcome of interest (e.g., vote choice, rating, or utility), possibly subject to a penalty
that keeps the learned distribution close to the original design distribution. It can also
incorporate institutional rules (e.g., primaries, multiple stages of choice) by specifying
additional arguments. Estimation can be done under standard generalized linear modeling assumptions
or more advanced approaches. The function returns estimates of the learned distribution and
the associated performance quantity (\eqn{Q(\boldsymbol{\pi}^\ast)}) along with optional inference based on
the (asymptotic) delta method.
}
\details{
\strong{Modeling the outcome:} Internally, \code{strategize} may fit a generalized linear model
or a more flexible approach (such as multi-cluster factorization) to learn the mapping from
factor-level assignments \code{W} (and optional covariates \code{X}) onto outcomes \code{Y}.
Once these outcome coefficients are estimated, the function uses gradient-based or closed-form
solutions to find the \emph{optimal stochastic intervention(s)}, i.e., new factor-level probability
distributions that maximize an expected outcome (or solve the max-min adversarial problem).

\strong{Adversarial or strategic design:} When \code{adversarial = TRUE}, the function attempts to
solve a zero-sum game in which one agent (say, \dQuote{A}) chooses its distribution to maximize
vote share, while the other (\dQuote{B}) simultaneously chooses its distribution to minimize
\dQuote{A}’s vote share. In many settings, \code{competing_group_variable_respondent} and related
arguments help define which respondents belong to the \dQuote{A} or \dQuote{B} sub-electorate
(e.g., a primary). The final solution is a mixed-strategy Nash equilibrium, if it exists, for
the forced-choice environment. This can be used to compare or interpret real-world candidate
positioning in multi-stage elections.

\strong{Regularization:} The argument \code{lambda} penalizes how far the learned distribution
strays from the baseline distribution \code{p_list}. This helps avoid overfitting in high-dimensional
designs. Different penalty types can be selected via \code{penalty_type}.

\strong{Implementation details:} Under the hood, this function may rely on \pkg{jax} for automatic
differentiation. By default, it uses an internal gradient-based approach. If \code{use_optax = TRUE},
the \code{optax} library is used for optimization. The function can automatically detect or
load a \pkg{conda} environment if specified, though advanced users can pass \code{conda_env_required = TRUE}
to enforce that environment activation is mandatory.
}
\examples{
\donttest{
# ============================================
# Example 1: Basic single-agent optimization
# ============================================
# Generate synthetic conjoint data
set.seed(42)
n <- 400  # Number of profiles (200 pairs)

# Factor matrix: candidate attributes
W <- data.frame(
  Gender = sample(c("Male", "Female"), n, replace = TRUE),
  Age = sample(c("Young", "Middle", "Old"), n, replace = TRUE),
  Party = sample(c("Dem", "Rep"), n, replace = TRUE)
)

# Simulate outcome: Female + Young candidates preferred
latent <- 0.3 * (W$Gender == "Female") +
          0.2 * (W$Age == "Young") -
          0.1 * (W$Age == "Old")
prob <- plogis(latent)

# Paired forced-choice: within each pair, one wins
pair_id <- rep(1:(n/2), each = 2)
Y <- numeric(n)
for (p in unique(pair_id)) {
  idx <- which(pair_id == p)
  winner <- sample(idx, 1, prob = prob[idx])
  Y[idx] <- as.integer(seq_along(idx) == which(idx == winner))
}
profile_order <- rep(1:2, n/2)

# Baseline probabilities (uniform assignment)
p_list <- list(
  Gender = c(Male = 0.5, Female = 0.5),
  Age = c(Young = 1/3, Middle = 1/3, Old = 1/3),
  Party = c(Dem = 0.5, Rep = 0.5)
)

# Run strategize to find optimal distribution
# (requires conda environment with JAX - see build_backend())
result <- strategize(
  Y = Y,
  W = W,
  lambda = 0.1,
  pair_id = pair_id,
  respondent_id = pair_id,
  respondent_task_id = pair_id,
  profile_order = profile_order,
  p_list = p_list,
  diff = TRUE,
  nSGD = 50,
  compute_se = FALSE
)

# View optimal distribution
print(result$pi_star_point)

# View expected outcome under optimal strategy
print(result$Q_point)
}

}
\seealso{
\code{\link{cv_strategize}} for cross-validation across candidate values of \code{lambda}.
See also \code{\link{strategize_onestep}} for a function that implements a \dQuote{one-step}
approach to M-estimation of the same target quantity.
}
