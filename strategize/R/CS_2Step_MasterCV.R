#' Cross-validation for Optimal Stochastic Interventions in Conjoint Analysis
#'
#' Performs cross-validation to select the regularization parameter \eqn{\lambda} 
#' (and, if desired, other hyperparameters) for the \code{\link{strategize}} function. 
#' This function splits the data by respondent (or user-specified units), trains
#' candidate models under a grid of \eqn{\lambda} values, and evaluates out-of-sample
#' performance, returning the model that maximizes a chosen criterion (e.g., out-of-sample 
#' expected utility or log-likelihood).
#'
#' @param Y A numeric or binary response vector. If binary (e.g., 0â€“1), it should 
#'   correspond to forced-choice outcomes (1 if candidate \code{A} is chosen; 0 if 
#'   candidate \code{B} is chosen). If numeric, please see details in 
#'   \code{\link{strategize}} for how outcomes are handled.
#' @param W A data frame or matrix representing the randomized conjoint attributes. 
#'   Each column is a factor or character vector indicating attribute levels for a 
#'   particular dimension. Multiple columns can be used if the conjoint has multiple
#'   attributes. 
#' @param X Optional covariate matrix or data frame for modeling systematic 
#'   heterogeneity. If \code{K > 1}, this is typically required for multi-class or 
#'   cluster-based models. Otherwise, set \code{X = NULL}.
#' @param lambda_seq A numeric vector of candidate \eqn{\lambda} values for 
#'   cross-validation. If \code{NULL} and \code{lambda} is also \code{NULL}, 
#'   a sequence of values is automatically generated (e.g., via 
#'   \code{10^seq(-4, 0, length.out = 5) * sd(Y)}).
#' @param lambda A single user-specified \eqn{\lambda} value. If provided, 
#'   cross-validation is effectively disabled unless \code{lambda_seq} is also 
#'   supplied. 
#' @param folds An integer or user-specified partitioning indicating the number of 
#'   cross-validation folds. Defaults to 2. See Details for how data splitting is done.
#' @param varcov_cluster_variable An optional clustering variable for robust standard 
#'   errors. For instance, if the data is from multiple respondents, specify respondent 
#'   IDs here for cluster-robust inference (via sandwich estimation). If \code{NULL}, no 
#'   cluster-based variance correction is used.
#' @param competing_group_variable_respondent Optional vector for multi-round or 
#'   multi-group setups, indicating which respondent belongs to which group. Used for 
#'   advanced or adversarial designs (e.g., dual-party contexts). If \code{NULL}, 
#'   standard usage is assumed.
#' @param competing_group_variable_candidate Similar to 
#'   \code{competing_group_variable_respondent}, but for candidate-level grouping. 
#'   If \code{NULL}, standard usage is assumed.
#' @param competing_group_competition_variable_candidate An optional variable for 
#'   specifying which candidate is in competition with which group. Relevant if 
#'   multi-step adversarial frameworks are used.
#' @param pair_id An optional vector (same length as \code{Y}) identifying which 
#'   rows (candidate pairs) belong to the same forced choice. For example, if each 
#'   respondent evaluates multiple pairs, this ID ensures correct grouping. 
#'   Required only in certain advanced difference-in-differences or paired analyses.
#' @param respondent_id A user-specified ID to denote respondent-level grouping, 
#'   typically used to cluster standard errors or to perform out-of-sample validation 
#'   by respondent. If \code{NULL}, a simple row index is used for splitting.
#' @param respondent_task_id Another optional ID for tasks (e.g., each respondent 
#'   might see multiple tasks). Helps in advanced designs. If \code{NULL}, ignored.
#' @param profile_order An optional vector capturing the ordering of candidate 
#'   profiles within tasks, if multiple profiles are being shown. Used in difference 
#'   or extended hierarchical modeling.
#' @param p_list A list of assignment probabilities for each attribute, if known 
#'   or desired as a baseline. If \code{NULL}, each level is assumed to have 
#'   uniform probability or derived from empirical frequencies in \code{W}.
#' @param slate_list An optional list specifying alternative or restricted sets of 
#'   attribute levels. Used when a subset of attributes is feasible or when bounding 
#'   certain strategies in an adversarial design.
#' @param use_optax Logical. If \code{TRUE}, uses the \pkg{optax} Python library 
#'   (via \pkg{reticulate}) for gradient-based optimization. If \code{FALSE}, uses 
#'   a default gradient-based approach from \pkg{jax}.
#' @param K An integer specifying the number of mixture components or clusters if 
#'   \code{X} is used (e.g., for multi-class analysis). Defaults to 1 (no mixture).
#' @param nSGD An integer number of iterations for gradient-based training. Defaults 
#'   to 100 but can be increased if convergence has not been reached.
#' @param diff Logical indicating whether a difference-based model (e.g., for 
#'   forced-choice or difference-in-outcomes) is used. Defaults to \code{FALSE}, but 
#'   set \code{TRUE} in certain difference-of-utility designs.
#' @param MaxMin Logical indicating whether to use a two-party or multi-agent 
#'   \emph{adversarial} approach in the optimization. If \code{TRUE}, a min-max 
#'   (zero-sum) formulation is employed. Defaults to \code{FALSE} (single-agent 
#'   or average-case optimization).
#' @param use_regularization Logical; if \code{TRUE}, penalty-based regularization 
#'   is used for the outcome model. Usually set to \code{TRUE} for large designs. 
#'   Defaults to \code{FALSE}.
#' @param force_gaussian Logical indicating whether a Gaussian family 
#'   (\code{lm}-style) is forced for the outcome model, even if \code{Y} is binary. 
#'   Defaults to \code{FALSE}.
#' @param a_init_sd A numeric controlling the random initialization scale for 
#'   unconstrained parameters in gradient-based optimization. Defaults to 0.001. 
#'   Larger values can help avoid local minima in complex outcome landscapes.
#' @param penalty_type A character string specifying the type of penalty for the 
#'   \emph{optimal stochastic intervention}, e.g., \code{"KL"}, \code{"L2"}, 
#'   or \code{"LogMaxProb"}. The default is \code{"KL"}.
#' @param compute_se Logical; if \code{TRUE}, attempts to compute standard errors 
#'   using M-estimation or the Delta method. Defaults to \code{TRUE}.
#' @param conda_env A character specifying the name of a Conda environment for 
#'   \pkg{reticulate}. If \code{NULL}, the default environment is used.
#' @param conda_env_required Logical. If \code{TRUE}, errors if the specified 
#'   Conda environment \code{conda_env} cannot be found. Otherwise tries to fall 
#'   back gracefully.
#' @param confLevel The confidence level (between 0 and 1) for interval estimation, 
#'   default 0.90.
#' @param nFolds_glm An integer specifying the number of folds in internal 
#'   regression-based cross-validation (if used) for outcome model selection. 
#'   Defaults to 3.
#' @param nMonte_MaxMin A positive integer specifying the number of Monte Carlo 
#'   draws for the min-max (adversarial) stage, if \code{MaxMin = TRUE}. Defaults 
#'   to 5.
#' @param nMonte_Qglm An integer specifying the number of Monte Carlo draws 
#'   for evaluating certain integrals in \code{glm}-based approximations, default 100.
#' @param optim_type A character describing the optimization routine. Typically 
#'   \code{"default"} uses a standard gradient-based approach; set \code{"tryboth"} 
#'   or \code{"SecondOrder"} for testing or advanced routines.
#'
#' @details
#' \code{strategize_cv} implements a cross-validation routine for 
#' \code{\link{strategize}}. First, the data is split into \code{folds} parts. 
#' For each fold, we train candidate outcome models and compute out-of-sample 
#' performance. The best-performing \eqn{\lambda} is selected. Finally, a 
#' refit on the full data is done using the chosen hyperparameters, returning 
#' the results of the final \code{\link{strategize}} call with \eqn{\lambda} set 
#' to the best value.
#'
#' The function supports a wide range of conjoints, including forced-choice 
#' (where \code{diff = TRUE}), multi-cluster outcome modeling (where \eqn{K > 1}), 
#' and adversarial designs (where \code{MaxMin = TRUE}). Regularization for the 
#' outcome model or for the candidate distribution can be enabled via 
#' \code{use_regularization} and \code{penalty_type}. Cross-validation is particularly 
#' helpful when the data is limited or highly dimensional.
#'
#' @return A named list with components:
#' \describe{
#'   \item{pi_star_point}{The estimated optimal probability distribution(s) over 
#'   candidate profiles (\eqn{\hat{\boldsymbol{\pi}}^*}).}
#'
#'   \item{Q_point_mEst}{The estimated expected outcome (e.g., vote share) 
#'   under the selected optimal distribution.}
#'
#'   \item{lambda}{The chosen \eqn{\lambda} value from cross-validation (and 
#'   any other relevant hyperparameters).}
#'
#'   \item{CVInfo}{A data frame or matrix summarizing cross-validation results, 
#'   e.g., in-sample and out-of-sample estimates for each candidate \eqn{\lambda}.}
#'
#'   \item{Other components}{Various additional objects useful for inference and 
#'   debugging (e.g., final model fits, standard error estimates, weighting 
#'   details).}
#' }
#'
#' @seealso 
#' \code{\link{strategize}} for direct optimization of stochastic interventions 
#' in conjoint analysis, including average and adversarial settings. 
#'
#' @examples
#' \donttest{
#' # A minimal example using hypothetical data
#' set.seed(123)
#' # Suppose Y is a binary forced choice outcome, W has several attributes (factors)
#' Y <- rbinom(200, size = 1, prob = 0.5)
#' W <- data.frame(
#'   Gender = sample(c("Male","Female"), 200, TRUE),
#'   Age    = sample(c("35","50","65"),  200, TRUE),
#'   Party  = sample(c("Dem","Rep"),     200, TRUE)
#' )
#'
#' # Cross-validate over a range of lambda
#' lam_seq <- c(0, 0.001, 0.01, 0.1)
#' cv_fit <- cv_strategize(
#'   Y = Y, 
#'   W = W, 
#'   lambda_seq = lam_seq, 
#'   folds = 2
#' )
#'
#' # Extract optimal lambda and final fit
#' print(cv_fit$lambda)
#' print(cv_fit$CVInfo)
#' print(names(cv_fit$pi_star_point))
#' }
#'
#' @export

cv_strategize       <-          function(
                                            Y,
                                            W,
                                            X = NULL,
                                            lambda_seq = NULL,
                                            lambda = NULL,
                                            folds = 2L,
                                            varcov_cluster_variable = NULL,
                                            competing_group_variable_respondent = NULL,
                                            competing_group_variable_candidate = NULL,
                                            competing_group_competition_variable_candidate = NULL,
                                            pair_id = NULL,
                                            respondent_id = NULL,
                                            respondent_task_id = NULL,
                                            profile_order = NULL,
                                            p_list = NULL,
                                            slate_list = NULL,
                                            use_optax = F,
                                            K = 1,
                                            nSGD = 100,
                                            diff = F, MaxMin = F,
                                            use_regularization = F,
                                            force_gaussian = F,
                                            a_init_sd = 0.001,
                                            penalty_type = "KL",
                                            compute_se = T,
                                            conda_env = NULL,
                                            conda_env_required = F,
                                            confLevel = 0.90,
                                            nFolds_glm = 3L,
                                            nMonte_MaxMin = 5L,
                                            nMonte_Qglm = 100L,
                                            optim_type = "gd"){
  # initialize environment
  if(!"jnp" %in% ls(envir = strenv)) {
    initialize_jax(conda_env = conda_env, conda_env_required = conda_env_required) 
  }

  # setup lamba 
  if(is.null(lambda_seq) & is.null(lambda)){
    lambda_seq <- 10^seq(-4, 0, length.out = 5) * sd(Y, na.rm = T)
  }
  if(is.null(lambda_seq) & !is.null(lambda)){ lambda_seq <- lambda }

  if(is.null(respondent_id)){ respondent_id <- 1:length(Y) }

  # CV sequence
  {
    message("Starting CV sequence...")
    outsamp_results <- insamp_results <- matrix(nrow = 0, ncol = 4, dimnames = list(NULL, c("lambda","Qhat","Qse","selected")))

    # build cv splits - same for all lambda 
    all_tabs <- apply(W,2,table)
    ok_counter <- 0; ok <- F; while(!ok){ 
        ok_counter <- ok_counter + 1 
        
        # sample based on unique respondent-tasks 
        tmp_ <- (paste0(respondent_id, "_", respondent_task_id))
        indi_list <- sample(1:folds, size = length(unique(tmp_)), replace = T)
        names(indi_list) <- unique(tmp_)
        indi_list <- indi_list[tmp_]
        
        indi_list <- sapply(1:folds, function(f_){
          list(which(!indi_list %in% f_), # pos 1 is in
               which(indi_list %in% f_) ) # pos 2 is out 
        })
        split_tabs_in <- apply(indi_list,2,function(l_){ apply(W[l_[[1]],],2,table) })
        if(all(names(unlist(all_tabs)) == names(unlist(split_tabs_in)))){
          if(all(unlist(split_tabs_in) > 10)){ ok <- T }
        }
        if(ok_counter > 1000){stop("Stopping: Could not find split with > 10 observations per factor level.")}
    }
    
    lambda_counter <- 0; for(lambda__ in lambda_seq){
      lambda_counter <- lambda_counter + 1
      Qoptimized__ <- replicate(n = folds, list())
      message(sprintf("At lambda %s of %s...", lambda_counter, length(lambda_seq)))

      # CV sequence
      q_vec_in <- q_vec_out <- c()
      for(split_ in c(1:folds)){
        out_indices <- indi_list[2,split_][[1]]; gc(); strenv$py_gc$collect()
        for(type_ in c(1,2)){ 
          # in sample optimization of pi*, evaluation on OOS coefficients 
          use_indices <- indi_list[type_,split_][[1]]
          nSGD_use <- ifelse(type_ == 1, yes = nSGD, no = 1L)
          
          # strategize call
          Qoptimized__[[split_]][[type_]] <- strategize(
  
            # input data
            Y = Y[use_indices],
            W = W[use_indices,],
            X = ifelse(analysisType=="cluster", yes = list(X[use_indices,]), no = list(NULL))[[1]],
            varcov_cluster_variable = varcov_cluster_variable[use_indices],
            pair_id = pair_id[use_indices],
            respondent_id = respondent_id[ use_indices ],
            respondent_task_id = respondent_task_id[ use_indices ],
            profile_order = profile_order[ use_indices ],
            p_list = p_list,
            slate_list = slate_list, 
            use_optax = use_optax, 
            lambda = lambda__,
  
            # hyperparameters
            compute_se = F, 
            nSGD = nSGD_use,
            penalty_type = penalty_type,
            K = K,
            force_gaussian = force_gaussian,
            use_regularization = use_regularization,
            optim_type = optim_type,
            a_init_sd = a_init_sd,
            nFolds_glm = nFolds_glm,
            diff = diff,
            MaxMin = MaxMin,
            conda_env = conda_env,
            conda_env_required = conda_env_required)
        }
        
        # out of sample test of pi* on new estimates 
        q_vec_in <- c(q_vec_in, Qoptimized__[[split_]][[1]]$Q_point)
        q_vec_out <- c(q_vec_out, unlist(Qoptimized__[[split_]][[2]]$Qfxn(
          "pi_star_ast" = Qoptimized__[[split_]][[1]]$pi_star_red_ast,
          "pi_star_dag" = Qoptimized__[[split_]][[1]]$pi_star_red_dag,
          "EST_INTERCEPT_tf_ast" = Qoptimized__[[split_]][[2]]$est_intercept_jnp,
          "EST_COEFFICIENTS_tf_ast" = Qoptimized__[[split_]][[2]]$est_coefficients_jnp,
          "EST_INTERCEPT_tf_dag" = Qoptimized__[[split_]][[2]]$est_intercept_jnp,
          "EST_COEFFICIENTS_tf_dag" = Qoptimized__[[split_]][[2]]$est_coefficients_jnp)$tolist()[[1]])
        )
      }
      outsamp_results <- as.data.frame(rbind(outsamp_results, c(lambda__, mean(q_vec_out), se(q_vec_out), 0)))
      insamp_results <- as.data.frame(rbind(insamp_results, c(lambda__, mean(q_vec_in), se(q_vec_in), 0)))
    }

    outsamp_results$l_bound <- outsamp_results$Qhat - (qStar_lambda <- 1) * outsamp_results$Qse
    outsamp_results$u_bound <- outsamp_results$Qhat + qStar_lambda * outsamp_results$Qse
    lambda__ <- lambda_seq[which_selected <- which.max(outsamp_results$Qhat)] # lambda.min rule
    #lambda__ <- lambda_seq[which_selected <- which(max(outsamp_results$Qhat <= outsamp_results$u_bound)[1]] # lambda.se rule
    outsamp_results$selected[which_selected] <- 1 
    message("Done with CV sequence & starting final run with log(lambda) of %.2f...", log(lambda__))
  }

  # final output
  gc(); strenv$py_gc$collect(); Qoptimized_ <- strategize(
                              # input data
                              Y = Y,
                              W = W,
                              X = ifelse(K > 1, yes = list(X), no = list(NULL))[[1]],
                              nSGD = nSGD,
                              penalty_type = penalty_type,
                              varcov_cluster_variable = varcov_cluster_variable,
                              competing_group_variable_respondent = competing_group_variable_respondent,
                              competing_group_variable_candidate = competing_group_variable_candidate,
                              competing_group_competition_variable_candidate = competing_group_competition_variable_candidate,
                              pair_id = pair_id,
                              respondent_id = respondent_id,
                              respondent_task_id = respondent_task_id,
                              profile_order = profile_order,
                              p_list = p_list,
                              slate_list = slate_list, 
                              use_optax = use_optax, 
                              lambda = lambda__, # this lambda is the one chosen via CV

                              # hyperparameters
                              optim_type = optim_type,
                              force_gaussian = force_gaussian,
                              use_regularization = use_regularization,
                              a_init_sd = a_init_sd,
                              compute_se = compute_se,
                              K = K,
                              nMonte_MaxMin = nMonte_MaxMin,
                              nFolds_glm = nFolds_glm,
                              diff = diff,
                              MaxMin = MaxMin,
                              conda_env = conda_env,
                              conda_env_required = conda_env_required)
  message("Done with strategic analysis!")
  return(  c( Qoptimized_,
            "lambda" = lambda__,
            "qStar_lambda" = qStar_lambda,
            "CVInfo" = list(outsamp_results )) )
}
