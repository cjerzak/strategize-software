% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CS_2Step_MasterCV.R
\name{cv_strategize}
\alias{cv_strategize}
\title{Cross-validation for Optimal Stochastic Interventions in Conjoint Analysis}
\usage{
cv_strategize(
  Y,
  W,
  X = NULL,
  lambda_seq = NULL,
  lambda = NULL,
  folds = 2L,
  varcov_cluster_variable = NULL,
  competing_group_variable_respondent = NULL,
  competing_group_variable_candidate = NULL,
  competing_group_competition_variable_candidate = NULL,
  pair_id = NULL,
  respondent_id = NULL,
  respondent_task_id = NULL,
  profile_order = NULL,
  p_list = NULL,
  slate_list = NULL,
  use_optax = F,
  K = 1,
  nSGD = 100,
  diff = F,
  adversarial = F,
  adversarial_model_strategy = "four",
  partial_pooling = NULL,
  partial_pooling_strength = 50,
  use_regularization = TRUE,
  force_gaussian = F,
  temperature = 0.5,
  a_init_sd = 0.001,
  learning_rate_max = 0.001,
  penalty_type = "KL",
  outcome_model_type = "glm",
  neural_mcmc_control = NULL,
  compute_se = T,
  conda_env = "strategize_env",
  conda_env_required = F,
  conf_level = 0.9,
  nFolds_glm = 3L,
  nMonte_adversarial = 5L,
  primary_pushforward = "mc",
  primary_strength = 1,
  primary_n_entrants = 1L,
  primary_n_field = 1L,
  nMonte_Qglm = 100L,
  optim_type = "gd",
  optimism = "extragrad",
  optimism_coef = 1,
  rain_gamma = 0.01,
  rain_eta = 0.001
)
}
\arguments{
\item{Y}{A numeric or binary response vector. If binary (e.g., 0â€“1), it should 
correspond to forced-choice outcomes (1 if candidate \code{A} is chosen; 0 if 
candidate \code{B} is chosen). If numeric, please see details in 
\code{\link{strategize}} for how outcomes are handled.}

\item{W}{A data frame or matrix representing the randomized conjoint attributes. 
Each column is a factor or character vector indicating attribute levels for a 
particular dimension. Multiple columns can be used if the conjoint has multiple
attributes.}

\item{X}{Optional covariate matrix or data frame for modeling systematic 
heterogeneity. If \code{K > 1}, this is typically required for multi-class or 
cluster-based models. Otherwise, set \code{X = NULL}.}

\item{lambda_seq}{A numeric vector of candidate \eqn{\lambda} values for 
cross-validation. If \code{NULL} and \code{lambda} is also \code{NULL}, 
a sequence of values is automatically generated (e.g., via 
\code{10^seq(-4, 0, length.out = 5) * sd(Y)}).}

\item{lambda}{A single user-specified \eqn{\lambda} value. If provided, 
cross-validation is effectively disabled unless \code{lambda_seq} is also 
supplied.}

\item{folds}{An integer or user-specified partitioning indicating the number of 
cross-validation folds. Defaults to 2. See Details for how data splitting is done.}

\item{varcov_cluster_variable}{An optional clustering variable for robust standard 
errors. For instance, if the data is from multiple respondents, specify respondent 
IDs here for cluster-robust inference (via sandwich estimation). If \code{NULL}, no 
cluster-based variance correction is used.}

\item{competing_group_variable_respondent}{Optional vector for multi-round or 
multi-group setups, indicating which respondent belongs to which group. Used for 
advanced or adversarial designs (e.g., dual-party contexts). If \code{NULL}, 
standard usage is assumed.}

\item{competing_group_variable_candidate}{Similar to 
\code{competing_group_variable_respondent}, but for candidate-level grouping. 
If \code{NULL}, standard usage is assumed.}

\item{competing_group_competition_variable_candidate}{An optional variable for 
specifying which candidate is in competition with which group. Relevant if 
multi-step adversarial frameworks are used.}

\item{pair_id}{An optional vector (same length as \code{Y}) identifying which 
rows (candidate pairs) belong to the same forced choice. For example, if each 
respondent evaluates multiple pairs, this ID ensures correct grouping. 
Required only in certain advanced difference-in-differences or paired analyses.}

\item{respondent_id}{A user-specified ID to denote respondent-level grouping, 
typically used to cluster standard errors or to perform out-of-sample validation 
by respondent. If \code{NULL}, a simple row index is used for splitting.}

\item{respondent_task_id}{Another optional ID for tasks (e.g., each respondent 
might see multiple tasks). Helps in advanced designs. If \code{NULL}, ignored.}

\item{profile_order}{An optional vector capturing the ordering of candidate 
profiles within tasks, if multiple profiles are being shown. Used in difference 
or extended hierarchical modeling.}

\item{p_list}{A list of assignment probabilities for each attribute, if known 
or desired as a baseline. If \code{NULL}, each level is assumed to have 
uniform probability or derived from empirical frequencies in \code{W}.}

\item{slate_list}{An optional list specifying alternative or restricted sets of 
attribute levels. Used when a subset of attributes is feasible or when bounding 
certain strategies in an adversarial design.}

\item{use_optax}{Logical. If \code{TRUE}, uses the \pkg{optax} Python library 
(via \pkg{reticulate}) for gradient-based optimization. If \code{FALSE}, uses 
a default gradient-based approach from \pkg{jax}.}

\item{K}{An integer specifying the number of mixture components or clusters if 
\code{X} is used (e.g., for multi-class analysis). Defaults to 1 (no mixture).}

\item{nSGD}{An integer number of iterations for gradient-based training. Defaults 
to 100 but can be increased if convergence has not been reached.}

\item{diff}{Logical indicating whether a difference-based model (e.g., for 
forced-choice or difference-in-outcomes) is used. Defaults to \code{FALSE}, but 
set \code{TRUE} in certain difference-of-utility designs.}

\item{adversarial}{Logical indicating whether to use a two-party or multi-agent 
\emph{adversarial} approach in the optimization. If \code{TRUE}, a min-max 
(zero-sum) formulation is employed. Defaults to \code{FALSE} (single-agent 
or average-case optimization).}

\item{adversarial_model_strategy}{Character string indicating whether to estimate
\code{"four"} outcome models (primary + general for each group), \code{"two"} outcome models
(one per group reused for both primary and general), or \code{"neural"} (Bayesian Transformer
models with party tokens; defaults to a single pooled model across groups and stages. Set
\code{neural_mcmc_control$n_bayesian_models = 2} to fit separate AST/DAG models) in adversarial
mode.}

\item{partial_pooling}{Logical indicating whether to partially pool (shrink) group-specific
outcome model coefficients toward a shared average when using the "two" strategy. When
\code{NULL}, defaults to \code{TRUE} in the two-strategy adversarial case.}

\item{partial_pooling_strength}{Numeric scalar controlling the amount of shrinkage used for
partial pooling in the two-strategy adversarial case.}

\item{use_regularization}{Logical; if \code{TRUE}, penalty-based regularization
is used for the outcome model. Usually set to \code{TRUE} for large designs.
Defaults to \code{TRUE}.}

\item{force_gaussian}{Logical indicating whether a Gaussian family
(\code{lm}-style) is forced for the outcome model, even if \code{Y} is binary.
Defaults to \code{FALSE}.}

\item{temperature}{Optional numeric temperature controlling the smoothness of
Gumbel-Softmax sampling when exploring probability vectors. Smaller values
lead to distributions closer to the argmax. Defaults to \code{NULL}, which
allows internal defaults.}

\item{a_init_sd}{A numeric controlling the random initialization scale for
unconstrained parameters in gradient-based optimization. Defaults to 0.001.
Larger values can help avoid local minima in complex outcome landscapes.}

\item{learning_rate_max}{Base learning rate for gradient-based optimizers.
Defaults to \code{0.001}.}

\item{penalty_type}{A character string specifying the type of penalty for the
\emph{optimal stochastic intervention}, e.g., \code{"KL"}, \code{"L2"},
or \code{"LogMaxProb"}. The default is \code{"KL"}.}

\item{outcome_model_type}{Character string indicating the outcome model to
use, such as \code{"glm"} for generalized linear models or \code{"neural"}
for a neural-network approximation. Defaults to \code{"glm"}.}

\item{neural_mcmc_control}{Optional list overriding default MCMC settings used when
\code{outcome_model_type = "neural"}. Use
\code{neural_mcmc_control$uncertainty_scope = "output"} to restrict delta-method
uncertainty to output-layer parameters. In adversarial neural mode, set
\code{neural_mcmc_control$n_bayesian_models = 2} to fit separate AST/DAG models
(default is 1 for a single differential model). Use
\code{neural_mcmc_control$ModelDims} and \code{neural_mcmc_control$ModelDepth}
to override the Transformer hidden width and depth. Set
\code{neural_mcmc_control$cross_candidate_encoder = "term"} (or \code{TRUE}) to include
the opponent-dependent cross-candidate term in pairwise mode, or set
\code{neural_mcmc_control$cross_candidate_encoder = "full"} to enable a full cross-encoder
that jointly encodes both candidates. Use \code{"none"} (or \code{FALSE}) to disable.
For variational inference (subsample_method = "batch_vi"), set
\code{neural_mcmc_control$optimizer} to \code{"adam"} (numpyro.optim),
\code{"adamw"} (AdamW), \code{"adabelief"} (optax), or \code{"muon"} (optax.contrib). Learning-rate decay is controlled by
\code{neural_mcmc_control$svi_steps} (integer steps, or \code{"optimal"} for
a scaling-law heuristic based on model/data size; for minibatched VI this
also scales with \code{batch_size}) and
\code{neural_mcmc_control$svi_lr_schedule} (default \code{"warmup_cosine"}), with optional
\code{svi_lr_warmup_frac} and \code{svi_lr_end_factor}.}

\item{compute_se}{Logical; if \code{TRUE}, attempts to compute standard errors
using M-estimation or the Delta method. Defaults to \code{TRUE}.}

\item{conda_env}{A character specifying the name of a Conda environment for
\pkg{reticulate}. Defaults to \code{"strategize_env"}.}

\item{conda_env_required}{Logical. If \code{TRUE}, errors if the specified 
Conda environment \code{conda_env} cannot be found. Otherwise tries to fall 
back gracefully.}

\item{conf_level}{The confidence level (between 0 and 1) for interval estimation, 
default 0.90.}

\item{nFolds_glm}{An integer specifying the number of folds in internal 
regression-based cross-validation (if used) for outcome model selection. 
Defaults to 3.}

\item{nMonte_adversarial}{A positive integer specifying the number of Monte Carlo 
draws for the min-max (adversarial) stage, if \code{adversarial = TRUE}. Defaults 
to 5.}

\item{primary_pushforward}{Character string controlling the primary-stage push-forward estimator.
Use \code{"mc"} (default) for Monte Carlo sampling with per-draw primary winners, or
\code{"linearized"} for the faster averaged-weight approximation, or
\code{"multi"} for multi-candidate primaries.}

\item{primary_strength}{Numeric scalar controlling primary decisiveness (see \code{\link{strategize}}).}

\item{primary_n_entrants}{Integer number of entrant candidates per party in multi-candidate primaries.}

\item{primary_n_field}{Integer number of field candidates per party in multi-candidate primaries.}

\item{nMonte_Qglm}{An integer specifying the number of Monte Carlo draws 
for evaluating certain integrals in \code{glm}-based approximations, default 100.}

\item{optim_type}{A character describing the optimization routine. Typically 
\code{"default"} uses a standard gradient-based approach; set \code{"tryboth"} 
or \code{"SecondOrder"} for testing or advanced routines.}

\item{optimism}{Character string controlling optimistic / extra-gradient updates for the
gradient optimizer. Options: \code{"extragrad"} (default), \code{"smp"} (stochastic mirror-prox),
\code{"ogda"}, \code{"rain"} (RAIN: Recursive Anchored Iteration with anchored extra-gradient and
increasing quadratic anchor penalties),
or \code{"none"}. Only supported when \code{use_optax = FALSE}.}

\item{optimism_coef}{Numeric scalar controlling the magnitude of optimism adjustments. For
\code{"ogda"}, this scales the optimistic correction term. For \code{"rain"}, this is the
initial anchor weight \eqn{\lambda_0}; anchor weights grow multiplicatively by \eqn{(1+\gamma)}
each outer stage.}

\item{rain_gamma}{Non-negative numeric scalar for the RAIN anchor-growth parameter \eqn{\gamma}.
If not supplied, the default is auto-scaled downward when \code{nSGD} exceeds 100
to keep total anchor growth roughly constant. Only used when \code{optimism = "rain"}.}

\item{rain_eta}{Optional numeric scalar step size \eqn{\eta} for RAIN. Defaults to
\code{0.001} and is auto-scaled downward when \code{nSGD} exceeds 100 if not supplied.
Only used when \code{optimism = "rain"}.}
}
\value{
A named list with components:
\describe{
  \item{pi_star_point}{The estimated optimal probability distribution(s) over 
  candidate profiles (\eqn{\hat{\boldsymbol{\pi}}^*}).}

  \item{Q_point_mEst}{The estimated expected outcome (e.g., vote share) 
  under the selected optimal distribution.}

  \item{lambda}{The chosen \eqn{\lambda} value from cross-validation (and 
  any other relevant hyperparameters).}

  \item{CVInfo}{A data frame or matrix summarizing cross-validation results, 
  e.g., in-sample and out-of-sample estimates for each candidate \eqn{\lambda}.}

  \item{Other components}{Various additional objects useful for inference and 
  debugging (e.g., final model fits, standard error estimates, weighting 
  details).}
}
}
\description{
Performs cross-validation to select the regularization parameter \eqn{\lambda} 
(and, if desired, other hyperparameters) for the \code{\link{strategize}} function. 
This function splits the data by respondent (or user-specified units), trains
candidate models under a grid of \eqn{\lambda} values, and evaluates out-of-sample
performance, returning the model that maximizes a chosen criterion (e.g., out-of-sample 
expected utility or log-likelihood).
}
\details{
\code{cv_strategize} implements a cross-validation routine for
\code{\link{strategize}}. First, the data is split into \code{folds} parts. 
For each fold, we train candidate outcome models and compute out-of-sample 
performance. The best-performing \eqn{\lambda} is selected. Finally, a 
refit on the full data is done using the chosen hyperparameters, returning 
the results of the final \code{\link{strategize}} call with \eqn{\lambda} set 
to the best value.

The function supports a wide range of conjoints, including forced-choice 
(where \code{diff = TRUE}), multi-cluster outcome modeling (where \eqn{K > 1}), 
and adversarial designs (where \code{adversarial = TRUE}). Regularization for the 
outcome model or for the candidate distribution can be enabled via 
\code{use_regularization} and \code{penalty_type}. Cross-validation is particularly 
helpful when the data is limited or highly dimensional.
}
\examples{
\donttest{
# ================================================
# Cross-validation to select regularization lambda
# ================================================
set.seed(123)
n <- 400  # profiles (200 pairs)

# Generate factor matrix
W <- data.frame(
  Gender = sample(c("Male", "Female"), n, replace = TRUE),
  Age = sample(c("35", "50", "65"), n, replace = TRUE),
  Party = sample(c("Dem", "Rep"), n, replace = TRUE)
)

# Simulate outcome with true effects
latent <- 0.2 * (W$Gender == "Female") + 0.15 * (W$Age == "35")
prob <- plogis(latent)

# Create paired forced-choice structure
pair_id <- rep(1:(n/2), each = 2)
Y <- numeric(n)
for (p in unique(pair_id)) {
  idx <- which(pair_id == p)
  winner <- sample(idx, 1, prob = prob[idx])
  Y[idx] <- as.integer(seq_along(idx) == which(idx == winner))
}
profile_order <- rep(1:2, n/2)

# Cross-validate over lambda values
# Lower lambda = less regularization = further from baseline
cv_result <- cv_strategize(
  Y = Y,
  W = W,
  lambda_seq = c(0.01, 0.1, 0.5, 1.0),
  folds = 2,
  pair_id = pair_id,
  respondent_id = pair_id,
  profile_order = profile_order,
  diff = TRUE,
  nSGD = 50,
  compute_se = FALSE
)

# View CV results and selected lambda
print(cv_result$lambda)       # Optimal lambda
print(cv_result$CVInfo)       # Performance at each lambda
print(cv_result$pi_star_point) # Optimal distribution
print(cv_result$Q_point)       # Expected outcome
}

}
\seealso{
\code{\link{strategize}} for direct optimization of stochastic interventions 
in conjoint analysis, including average and adversarial settings.
}
