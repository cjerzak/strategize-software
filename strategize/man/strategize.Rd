% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/two_step_master.R
\name{strategize}
\alias{strategize}
\title{Estimate Optimal (or Adversarial) Stochastic Interventions for Conjoint Experiments}
\usage{
strategize(
  Y,
  W,
  X = NULL,
  lambda,
  varcov_cluster_variable = NULL,
  competing_group_variable_respondent = NULL,
  competing_group_variable_respondent_proportions = NULL,
  competing_group_variable_candidate = NULL,
  competing_group_competition_variable_candidate = NULL,
  pair_id = NULL,
  respondent_id = NULL,
  respondent_task_id = NULL,
  profile_order = NULL,
  p_list = NULL,
  slate_list = NULL,
  K = 1,
  nSGD = 100,
  diff = FALSE,
  adversarial = FALSE,
  adversarial_model_strategy = "four",
  include_stage_interactions = NULL,
  partial_pooling = NULL,
  partial_pooling_strength = 50,
  use_regularization = TRUE,
  force_gaussian = FALSE,
  a_init_sd = 0.001,
  outcome_model_type = "glm",
  neural_mcmc_control = NULL,
  penalty_type = "KL",
  compute_se = FALSE,
  se_method = c("full", "implicit"),
  conda_env = "strategize_env",
  conda_env_required = FALSE,
  conf_level = 0.90,
  nFolds_glm = 3L,
  folds = NULL,
  nMonte_adversarial = 5L,
  primary_pushforward = "mc",
  primary_strength = 1.0,
  primary_n_entrants = 1L,
  primary_n_field = 1L,
  nMonte_Qglm = 100L,
  learning_rate_max = 0.001,
  temperature = 0.5, 
  save_outcome_model = FALSE,
  presaved_outcome_model = FALSE,
  outcome_model_key = NULL,
  use_optax = FALSE,
  optim_type = "gd",
  optimism = "extragrad",
  optimism_coef = 1,
  rain_lambda = 1,
  rain_gamma = 0.01,
  rain_L = NULL,
  rain_eta = 0.001,
  rain_variant = "alg10_staged",
  rain_output = "last",
  compute_hessian = TRUE,
  hessian_max_dim = 50L
)
}
\arguments{
\item{Y}{A numeric or binary vector of observed outcomes, typically in \code{\{0,1\}} for forced-choice
conjoint tasks, indicating whether the profile was selected. For instance, \code{Y = 1} if
candidate A was chosen over candidate B, and \code{Y = 0} otherwise. The length must match
the number of rows in \code{W}.}

\item{W}{A matrix or data frame representing the assigned levels of each factor in a conjoint
design (one column per factor). Each row corresponds to a single profile. For forced-choice
tasks, a given respondent may have contributed multiple rows if you reshape pairwise choices
into long format. If the experiment used multiple factors \eqn{D}, with each factor having
\eqn{L_d} levels, \code{W} should capture all factor assignments accordingly.}

\item{X}{An optional matrix or data frame of additional covariates, often respondent-level
features (e.g., respondent demographics). If \code{K > 1}, \code{X} may be used internally to
fit multi-cluster or multi-component outcome models, or to allow cluster-specific effect
estimation for more granular insights. Defaults to \code{NULL}.}

\item{lambda}{A numeric scalar or vector giving the regularization penalty (e.g., in Kullback-Leibler
or L2 sense) used to shrink the learned probability distribution(s) of factor levels toward a
baseline distribution \code{p_list}. Typically set via either domain knowledge or cross-validation.}

\item{varcov_cluster_variable}{An optional vector of cluster identifiers (e.g., respondent IDs)
used to form a robust variance-covariance estimate of the outcome model. If \code{NULL},
the usual IID assumption is made. Defaults to \code{NULL}.}

\item{competing_group_variable_respondent}{Optional variable marking competition group
membership of respondents. Particularly relevant in adversarial settings
(\code{adversarial = TRUE}) or multi-stage electoral settings, e.g., capturing the party of each
respondent. Defaults to \code{NULL}.}

\item{competing_group_variable_respondent_proportions}{Optional numeric vector specifying
the population proportions of each competing group. If \code{NULL}, proportions are estimated
from the data. Useful when the sample proportions differ from the target population proportions.
Defaults to \code{NULL}.}

\item{competing_group_variable_candidate}{Optional variable marking competition group
membership of candidate profiles. Defaults to \code{NULL}.}

\item{competing_group_competition_variable_candidate}{Optional variable indicating whether
a candidate profile belongs to the competing group in adversarial settings. Defaults to \code{NULL}.}

\item{pair_id}{A factor or numeric vector identifying the forced-choice pair. If each row of
\code{W} is a single profile, \code{pair_id} groups the rows belonging to the same choice set.
Defaults to \code{NULL}.}

\item{respondent_id, respondent_task_id}{Another set of optional identifiers. \code{respondent_id}
marks each respondent across tasks, while \code{respondent_task_id} can define unique IDs for
repeated measurements from the same respondent across multiple tasks. Useful for advanced
clustering or robust SEs. Defaults to \code{NULL}.}

\item{profile_order}{If each forced-choice is shown with different ordering (e.g., \verb{Candidate A}
vs. \verb{Candidate B}), \code{profile_order} can label each row accordingly. Helpful for ensuring
consistent labeling of reference vs. opposing profiles. Defaults to \code{NULL}.}

\item{p_list}{An optional list describing the baseline probability distribution over factor levels
in \code{W}. Typically derived from the initial design distribution or uniform assignment
distribution. If \code{NULL}, the function may assume uniform or attempt to estimate the
distribution from \code{W}.}

\item{slate_list}{An optional list (or lists) providing custom slates of candidate features
(and their associated probabilities). Used in more advanced or adversarial setups where
certain combinations must be included or excluded. If \code{NULL}, no special constraints
beyond the usual factor-level distributions are applied.}

\item{K}{Integer specifying the number of latent clusters for multi-component outcome models. If
\code{K = 1}, no latent clustering is done. Defaults to \code{1}.}

\item{nSGD}{Integer specifying the number of stochastic gradient descent (or gradient-based)
iterations to use when learning the optimal distributions. Defaults to \code{100}.}

\item{diff}{Logical indicating whether the outcome \code{Y} represents a first-difference or
difference-based metric. In forced-choice contexts, typically \code{diff = FALSE}. Defaults
to \code{FALSE}.}

\item{adversarial}{Logical controlling whether to enable the max-min adversarial scenario. When
\code{TRUE}, the function searches for a pair of distributions (one for each competing party
or group) such that each party's distribution is optimal given the other party's distribution.
Defaults to \code{FALSE}.}

\item{adversarial_model_strategy}{Character string indicating whether to estimate
\code{"four"} outcome models (primary + general for each group), \code{"two"} outcome models
(one per group reused for both primary and general), or \code{"neural"} (Bayesian Transformer
models with party tokens; defaults to a single pooled model across groups and stages. Set
\code{neural_mcmc_control$n_bayesian_models = 2} to fit separate AST/DAG models). Defaults to
\code{"four"}.}

\item{include_stage_interactions}{Logical indicating whether to include stage (primary vs
general) indicator and stage-by-factor interactions in the outcome model. When \code{NULL}
(default), automatically set to \code{TRUE} when \code{adversarial_model_strategy = "two"}
and \code{FALSE} otherwise. Including stage interactions allows a single pooled model to
learn different response patterns for primary vs general election scenarios, which helps
prevent pattern-matching equilibria where both parties converge to identical strategies.}

\item{partial_pooling}{Logical indicating whether to partially pool (shrink) group-specific
outcome model coefficients toward a shared average when
\code{adversarial_model_strategy = "two"}. When \code{NULL} (default), automatically set to
\code{TRUE} for the two-strategy adversarial case and \code{FALSE} otherwise.}

\item{partial_pooling_strength}{Numeric scalar controlling the amount of shrinkage used for
partial pooling in the two-strategy adversarial case. Interpreted as a pseudo-sample size:
larger values increase pooling, smaller values preserve group differentiation. Defaults to
\code{50}.}

\item{use_regularization}{Logical indicating whether to regularize the outcome model (in addition
to any penalty \code{lambda} on the distribution shift). This can help avoid overfitting in
high-dimensional designs. Defaults to \code{TRUE}.}

\item{force_gaussian}{Logical indicating whether to force a Gaussian-based outcome modeling
approach, even if \code{Y} is binary or forced-choice. If \code{FALSE}, the function attempts
to choose a more appropriate link (e.g., \code{"binomial"}). Defaults to \code{FALSE}.}

\item{a_init_sd}{Numeric scalar specifying the standard deviation for random initialization
of unconstrained parameters used in the gradient-based search over factor-level probabilities.
Defaults to \code{0.001}.}

\item{outcome_model_type}{Character string specifying the outcome model to use. Currently
supports \code{"glm"} for generalized linear models or \code{"neural"} for a neural-network
approximation. Defaults to \code{"glm"}.}

\item{neural_mcmc_control}{Optional list overriding default MCMC settings used when
\code{outcome_model_type = "neural"}. Named entries override the defaults in
\code{two_step_model_outcome_neural.R}. Set
\code{neural_mcmc_control$uncertainty_scope = "output"} to compute delta-method
uncertainty using only the output-layer parameters (default is \code{"all"}). In adversarial
neural mode, set \code{neural_mcmc_control$n_bayesian_models = 2} to fit separate AST/DAG
models (default is 1 for a single differential model). Use
\code{neural_mcmc_control$ModelDims} and \code{neural_mcmc_control$ModelDepth} to override
the Transformer hidden width and depth. Set
\code{neural_mcmc_control$cross_candidate_encoder = "term"} (or \code{TRUE}) to include
the opponent-dependent cross-candidate term in pairwise mode, set
\code{neural_mcmc_control$cross_candidate_encoder = "attn"} to add a lightweight
cross-attention step, or set \code{neural_mcmc_control$cross_candidate_encoder = "full"}
to enable a full cross-encoder that jointly encodes both candidates. Use
\code{"none"} (or \code{FALSE}) to disable.
For variational inference (subsample_method = "batch_vi"), set
\code{neural_mcmc_control$optimizer} to \code{"adam"} (numpyro.optim),
\code{"adamw"} (AdamW), \code{"adabelief"} (optax), or \code{"muon"} (optax.contrib). Learning-rate decay is controlled by
\code{neural_mcmc_control$svi_steps} (integer steps, or \code{"optimal"} for
a scaling-law heuristic based on model/data size; for minibatched VI this
also scales with \code{batch_size}) and
\code{neural_mcmc_control$svi_lr_schedule} (default \code{"warmup_cosine"}), with optional
\code{svi_lr_warmup_frac} and \code{svi_lr_end_factor}.}

\item{penalty_type}{A character string specifying the type of penalty (e.g., \code{"KL"}, \code{"L2"},
or \code{"LogMaxProb"}) used in the objective function for shifting the factor-level probabilities
away from the baseline \code{p_list}. Defaults to \code{"KL"}.}

\item{compute_se}{Logical indicating whether standard errors should be computed for the final
estimates (via the delta method or related expansions). Defaults to \code{FALSE}.}

\item{se_method}{Character string specifying the SE computation method when \code{compute_se = TRUE}.
\code{"full"} differentiates through the full optimization trace (default). \code{"implicit"}
uses implicit differentiation at the solution (adversarial equilibrium or non-adversarial optimum).}

\item{conda_env}{A character string naming a Python conda environment that includes \pkg{jax},
\pkg{optax}, and other dependencies. If not \code{NULL}, the function attempts to activate
that environment. Defaults to \code{"strategize_env"}.}

\item{conda_env_required}{Logical; if \code{TRUE}, raises an error if the environment given by
\code{conda_env} cannot be activated. Otherwise, the function attempts to proceed with any
available installation. Defaults to \code{FALSE}.}

\item{conf_level}{Numeric in \eqn{(0,1)}, specifying the confidence level for intervals or
credible bounds. Defaults to \code{0.90}.}

\item{nFolds_glm}{Integer specifying the number of folds (default \code{3L}) for internal
cross-validation used in certain outcome model or regularization steps. Defaults to \code{3L}.}

\item{folds}{An optional user-supplied partitioning or CV scheme, overriding \code{nFolds_glm}.
Defaults to \code{NULL}.}

\item{nMonte_adversarial}{Integer specifying the number of Monte Carlo samples used in adversarial
or max-min steps, e.g., sampling from the opposing candidate's distribution to approximate
expected payoffs. Defaults to \code{5L}.}

\item{primary_pushforward}{Character string controlling the primary-stage push-forward estimator.
Use \code{"mc"} (default) for Monte Carlo sampling with per-draw primary winners, or
\code{"linearized"} for the faster averaged-weight approximation, or
\code{"multi"} for multi-candidate primaries.}

\item{primary_strength}{Numeric scalar controlling primary decisiveness. Values > 1 make
primary outcomes more deterministic; values in (0, 1) make primaries more noisy. Defaults
to 1.0 (neutral scaling).}

\item{primary_n_entrants}{Integer number of entrant candidates sampled per party in
multi-candidate primaries (\code{primary_pushforward = "multi"}). Defaults to 1.}

\item{primary_n_field}{Integer number of field candidates sampled per party in
multi-candidate primaries (\code{primary_pushforward = "multi"}). Defaults to 1.}

\item{nMonte_Qglm}{Integer specifying the number of Monte Carlo samples for evaluating or
approximating the quantity of interest under certain outcomes or distributions. Defaults to
\code{100L}.}

\item{learning_rate_max}{Base learning rate for gradient-based optimizers. Defaults to
\code{0.001}.}

\item{temperature}{Numeric temperature parameter used in Gumbel-Softmax sampling to smooth
the exploration of the probability simplex. Smaller values yield distributions closer to the
argmax. Defaults to \code{0.5}.}

\item{save_outcome_model}{Logical indicating whether to save the fitted outcome model to
disk for reuse. Useful for large models or repeated runs. Defaults to \code{FALSE}.}

\item{presaved_outcome_model}{Logical indicating whether to use a previously saved outcome
model instead of re-fitting. Defaults to \code{FALSE}.}

\item{outcome_model_key}{Optional character string to append to saved outcome model
filenames. Useful for distinguishing between different model configurations or
experimental runs. When provided, files are saved as
\code{main_{group}_{round}_{key}.csv}. Defaults to \code{NULL}.}

\item{use_optax}{Logical indicating whether to use the \href{https://github.com/google-deepmind/optax}{\code{optax}}
library for gradient-based optimization in JAX (\code{TRUE}) or a built-in method (\code{FALSE}).
Defaults to \code{FALSE}.}

\item{optim_type}{A character string for choosing which optimizer or approach is used internally
(e.g., \code{"gd"} for gradient descent). Defaults to \code{"gd"}.}

\item{optimism}{Character string controlling optimistic / extra-gradient updates for the gradient
optimizer. Options: \code{"extragrad"} (default; classic Korpelevich extra-gradient),
\code{"smp"} (stochastic mirror-prox: extra-gradient with weighted averaging of look-ahead points),
\code{"ogda"} (optimistic gradient), \code{"rain"} (RAIN: Recursive Anchored Iteration with
anchored extra-gradient and increasing quadratic anchor penalties), or \code{"none"}
(standard updates). Only supported when
\code{use_optax = FALSE}.}

\item{optimism_coef}{Numeric scalar controlling the magnitude of optimism adjustments. For
\code{"ogda"}, this scales the optimistic correction term. Ignored when
\code{optimism = "rain"}.}

\item{rain_lambda}{Numeric scalar giving the base RAIN regularization scale \eqn{\lambda}.
The staged algorithm uses \eqn{\lambda_0 = \gamma \lambda} and grows by
\eqn{(1+\gamma)} each stage. Only used when \code{optimism = "rain"}.}

\item{rain_gamma}{Non-negative numeric scalar for the RAIN anchor-growth parameter \eqn{\gamma}.
Larger values grow anchor weights faster. If not supplied, the default is
auto-scaled downward when \code{nSGD} exceeds 100 to keep total anchor growth
roughly constant. Only used when \code{optimism = "rain"}.}

\item{rain_L}{Optional numeric scalar for the Lipschitz constant \eqn{L} used by RAIN.
When supplied and \code{rain_eta} is missing, \code{rain_eta} defaults to
\eqn{1/(8L)} and stage lengths follow \eqn{T_s = 16L/\lambda_s}. If \code{NULL},
\eqn{L} is estimated from \code{rain_eta}. Only used when \code{optimism = "rain"}.}

\item{rain_eta}{Optional numeric scalar step size \eqn{\eta} for RAIN. Defaults to
\code{0.001} and is auto-scaled downward when \code{nSGD} exceeds 100 if not
supplied. If \code{rain_L} is supplied and \code{rain_eta} is missing, defaults
to \eqn{1/(8L)}. Only used when \code{optimism = "rain"}.}

\item{rain_variant}{Character string specifying the RAIN variant. Options:
\code{"alg10_staged"} (merged Algorithm 10 with recursive stage anchors; default) or
\code{"alg9_single_loop"} (single-loop variant; not yet implemented). Only used when
\code{optimism = "rain"}.}

\item{rain_output}{Character string controlling the stage output for RAIN.
\code{"uniform_half"} samples uniformly from half-iterates within the stage (most faithful);
\code{"last"} returns the last iterate (default). Only used when \code{optimism = "rain"}.}

\item{compute_hessian}{Logical. Whether to compute Hessian functions for equilibrium
geometry analysis in adversarial mode. When \code{TRUE} (default), Hessian functions
are JIT-compiled to enable \code{\link{check_hessian_geometry}} analysis. Set to
\code{FALSE} to skip Hessian computation for faster execution.}

\item{hessian_max_dim}{Integer. Maximum number of parameters per player before
automatically skipping Hessian computation. Defaults to \code{50L}. For problems
with more parameters, Hessian computation is skipped to avoid memory/time overhead.
The result will have \code{hessian_skipped_reason = "high_dimension"} in this case.}
}
\value{
A named \code{list} containing:
\describe{
\item{\code{pi_star_point}}{An estimate of the (possibly multi-cluster or adversarial)
optimal distribution(s) over the factor levels.

Structure depends on parameters:
\itemize{
\item If \code{adversarial = TRUE} and \code{K = 1}, returns a pair of distributions (e.g., maximin solutions).
\item If \code{K > 1}, returns a list where each element corresponds to a cluster-optimal distribution.
\item Otherwise, returns a single distribution.}
}

\item{\code{pi_star_se}}{Standard errors for entries in \code{pi_star_point}. Mirrors the structure of \code{pi_star_point} (e.g., a pair of SEs if \code{adversarial = TRUE} and \code{K = 1}). Only present if \code{compute_se = TRUE}.}

\item{\code{Q_point_mEst}}{Point estimate(s) of the optimized outcome (e.g., utility/vote share). Matches the structure of \code{pi_star_point}.}

\item{\code{Q_se_mEst}}{Standard errors for \code{Q_point_mEst}. Only present if \code{compute_se = TRUE}.}

\item{\code{pi_star_lb}, \code{pi_star_ub}}{Confidence bounds for \code{pi_star_point} (if \code{compute_se = TRUE} and a confidence level is provided).}

\item{\code{outcome_model_view}}{Interpretable summaries of the fitted outcome models (by player and stage).
Includes main-effect tables and top interactions for AST/DAG primary/general submodels when available.}

\item{\code{CVInfo}}{Cross-validation performance data (if applicable). Typically a \code{data.frame} or list.}

\item{\code{estimationType}}{String indicating the approach used (e.g., \code{"TwoStep"} or \code{"OneStep"}).}

\item{\code{...}}{Additional internal details (e.g., fitted models, optimization logs).}
}
}
\description{
\code{strategize} implements the core methods described in the accompanying paper
for learning an optimal or adversarial probability distribution over conjoint factor levels.
It is specifically designed for forced-choice conjoint settings (e.g., candidate-choice experiments)
and can accommodate scenarios in which a single agent optimizes its strategy in isolation,
or in which two (potentially adversarial) agents simultaneously optimize against each other.

This function can be used to find the \emph{optimal stochastic intervention} for maximizing
an outcome of interest (e.g., vote choice, rating, or utility), possibly subject to a penalty
that keeps the learned distribution close to the original design distribution. It can also
incorporate institutional rules (e.g., primaries, multiple stages of choice) by specifying
additional arguments. Estimation can be done under standard generalized linear modeling assumptions
or more advanced approaches. The function returns estimates of the learned distribution and
the associated performance quantity (\eqn{Q(\boldsymbol{\pi}^\ast)}) along with optional inference based on
the (asymptotic) delta method.
}
\details{
\strong{Modeling the outcome:} Internally, \code{strategize} may fit a generalized linear model
or a more flexible approach (such as multi-cluster factorization) to learn the mapping from
factor-level assignments \code{W} (and optional covariates \code{X}) onto outcomes \code{Y}.
Once these outcome coefficients are estimated, the function uses gradient-based or closed-form
solutions to find the \emph{optimal stochastic intervention(s)}, i.e., new factor-level probability
distributions that maximize an expected outcome (or solve the max-min adversarial problem).

\strong{Adversarial or strategic design:} When \code{adversarial = TRUE}, the function attempts to
solve a zero-sum game in which one agent (say, \dQuote{A}) chooses its distribution to maximize
vote share, while the other (\dQuote{B}) simultaneously chooses its distribution to minimize
\dQuote{A}'s vote share. In many settings, \code{competing_group_variable_respondent} and related
arguments help define which respondents belong to the \dQuote{A} or \dQuote{B} sub-electorate
(e.g., a primary). The final solution is a mixed-strategy Nash equilibrium, if it exists, for
the forced-choice environment. This can be used to compare or interpret real-world candidate
positioning in multi-stage elections.

\strong{Regularization:} The argument \code{lambda} penalizes how far the learned distribution
strays from the baseline distribution \code{p_list}. This helps avoid overfitting in high-dimensional
designs. Different penalty types can be selected via \code{penalty_type}.

\strong{Implementation details:} Under the hood, this function may rely on \pkg{jax} for automatic
differentiation. By default, it uses an internal gradient-based approach. If \code{use_optax = TRUE},
the \code{optax} library is used for optimization. The function can automatically detect or
load a \pkg{conda} environment if specified, though advanced users can pass \code{conda_env_required = TRUE}
to enforce that environment activation is mandatory.
}
\examples{
\donttest{
# ============================================
# Example 1: Basic single-agent optimization
# ============================================
# Generate synthetic conjoint data
set.seed(42)
n <- 400  # Number of profiles (200 pairs)

# Factor matrix: candidate attributes
W <- data.frame(
  Gender = sample(c("Male", "Female"), n, replace = TRUE),
  Age = sample(c("Young", "Middle", "Old"), n, replace = TRUE),
  Party = sample(c("Dem", "Rep"), n, replace = TRUE)
)

# Simulate outcome: Female + Young candidates preferred
latent <- 0.3 * (W$Gender == "Female") +
          0.2 * (W$Age == "Young") -
          0.1 * (W$Age == "Old")
prob <- plogis(latent)

# Paired forced-choice: within each pair, one wins
pair_id <- rep(1:(n/2), each = 2)
Y <- numeric(n)
for (p in unique(pair_id)) {
  idx <- which(pair_id == p)
  winner <- sample(idx, 1, prob = prob[idx])
  Y[idx] <- as.integer(seq_along(idx) == which(idx == winner))
}
profile_order <- rep(1:2, n/2)

# Baseline probabilities (uniform assignment)
p_list <- list(
  Gender = c(Male = 0.5, Female = 0.5),
  Age = c(Young = 1/3, Middle = 1/3, Old = 1/3),
  Party = c(Dem = 0.5, Rep = 0.5)
)

# Run strategize to find optimal distribution
# (requires conda environment with JAX - see build_backend())
result <- strategize(
  Y = Y,
  W = W,
  lambda = 0.1,
  pair_id = pair_id,
  respondent_id = pair_id,
  respondent_task_id = pair_id,
  profile_order = profile_order,
  p_list = p_list,
  diff = TRUE,
  nSGD = 50,
  compute_se = FALSE
)

# View optimal distribution
print(result$pi_star_point)

# View expected outcome under optimal strategy
print(result$Q_point)
}

}
\seealso{
\code{\link{cv_strategize}} for cross-validation across candidate values of \code{lambda}.
See also \code{\link{strategize_onestep}} for a function that implements a \dQuote{one-step}
approach to M-estimation of the same target quantity.
}
